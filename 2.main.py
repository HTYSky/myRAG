# main.py (è°ƒè¯•ç‰ˆ)

import json
import faiss
import numpy as np
from fastapi import FastAPI
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer, CrossEncoder
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
import time
import os # å¼•å…¥osæ¨¡å—
import logging
from datetime import datetime

# --- 1. å…¨å±€å˜é‡ä¸æ¨¡å‹åŠ è½½ ---
app = FastAPI()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('api_logs.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Query(BaseModel):
    question: str
    option_A: str
    option_B: str
    option_C: str
    option_D: str

RAG_ASSETS_PATH = "./assets"
print("Loading chunks and metadata...")
with open(f"{RAG_ASSETS_PATH}/chunks.json", "r", encoding="utf-8") as f:
    CHUNKS = json.load(f)
with open(f"{RAG_ASSETS_PATH}/metadata.json", "r", encoding="utf-8") as f:
    METADATA = json.load(f)

# --- ä¸ºFAISSåŠ è½½è¿‡ç¨‹æ·»åŠ è¯¦ç»†çš„æ—¥å¿—åŸ‹ç‚¹ ---
print("\n--- Starting FAISS Index Loading ---")
FAISS_INDEX_FILE = f"{RAG_ASSETS_PATH}/knowledge_base.index"

try:
    # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(FAISS_INDEX_FILE):
        print(f"âŒ FATAL ERROR: FAISS index file not found at '{FAISS_INDEX_FILE}'")
        exit()
    
    print(f"âœ… [Step 1/5] Found FAISS index file at '{FAISS_INDEX_FILE}'.")
    
    # ä»ç¡¬ç›˜è¯»å–ç´¢å¼•
    INDEX = faiss.read_index(FAISS_INDEX_FILE)
    print(f"âœ… [Step 2/5] Successfully read index from disk. It contains {INDEX.ntotal} vectors.")

    # æ£€æŸ¥GPUç¯å¢ƒ
    if torch.cuda.is_available() and hasattr(faiss, "StandardGpuResources"):
        print("âœ… [Step 3/5] GPU detected and faiss-gpu seems installed.")
        
        # åˆå§‹åŒ–GPUèµ„æº
        res = faiss.StandardGpuResources()
        print("âœ… [Step 4/5] Successfully initialized GPU resources (StandardGpuResources).")
        
        # å°†ç´¢å¼•ç§»è‡³GPU
        INDEX = faiss.index_cpu_to_gpu(res, 1, INDEX) # ä½¿ç”¨1å·GPU
        print("âœ… [Step 5/5] Successfully moved FAISS index to GPU-0.")
        
    else:
        print("âš ï¸ [Step 3/5] GPU not detected or faiss-gpu not installed. Using CPU for FAISS index.")

except Exception as e:
    print(f"\nâŒ An error occurred during FAISS index loading:")
    import traceback
    traceback.print_exc()
    exit() # å¦‚æœå‡ºé”™åˆ™ç›´æ¥é€€å‡º

print("--- FAISS Index Loading Complete ---\n")
# --- ä¿®æ”¹ç»“æŸ ---


print("Loading embedding model...")
EMBEDDING_MODEL = SentenceTransformer('./model/bge-large-zh-v1.5', device='cuda')

print("Loading reranker model...")
RERANKER_MODEL = CrossEncoder('./model/bge-reranker-large', max_length=512, device='cuda')

print("Loading 4-bit quantized LLM with bitsandbytes...")
LLM_MODEL_PATH = './model/qwen2-7b-instruct-bnb-4bit'

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

LLM_MODEL = AutoModelForCausalLM.from_pretrained(
    LLM_MODEL_PATH,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)
TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_PATH, trust_remote_code=True)

TEXT_GENERATOR = pipeline(
    "text-generation",
    model=LLM_MODEL,
    tokenizer=TOKENIZER,
    device_map="auto"
)

# ... (åç»­çš„ search_and_rerank, build_prompt, å’Œ APIç«¯ç‚¹ä»£ç ä¿æŒä¸å˜) ...
def search_and_rerank(query_text, top_k_retrieve=50, top_k_rerank=3):
    logger.info(f"ğŸ” å¼€å§‹æ£€ç´¢å’Œé‡æ’åº - æŸ¥è¯¢æ–‡æœ¬: {query_text}")
    
    instruction = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
    query_embedding = EMBEDDING_MODEL.encode([instruction + query_text], normalize_embeddings=True)
    
    logger.info(f"ğŸ“Š å‘é‡æ£€ç´¢ - æ£€ç´¢å‰ {top_k_retrieve} ä¸ªæ–‡æ¡£")
    distances, indices = INDEX.search(query_embedding.astype('float32'), top_k_retrieve)
    
    if indices.size == 0:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£")
        return []
    
    retrieved_chunks = [CHUNKS[i] for i in indices[0]]
    logger.info(f"ğŸ“„ æ£€ç´¢åˆ° {len(retrieved_chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    
    # è®°å½•å‰5ä¸ªæ£€ç´¢ç»“æœçš„ç›¸ä¼¼åº¦åˆ†æ•°
    logger.info("ğŸ“ˆ æ£€ç´¢ç»“æœç›¸ä¼¼åº¦åˆ†æ•°:")
    for i, (dist, idx) in enumerate(zip(distances[0][:5], indices[0][:5])):
        logger.info(f"   {i+1}. ç´¢å¼•{idx}: è·ç¦»={dist:.4f}")
    
    pairs = [[query_text, chunk] for chunk in retrieved_chunks]
    scores = RERANKER_MODEL.predict(pairs)
    reranked_results = sorted(zip(scores, retrieved_chunks), key=lambda x: x[0], reverse=True)
    
    logger.info(f"ğŸ¯ é‡æ’åºå®Œæˆ - é€‰æ‹©å‰ {top_k_rerank} ä¸ªæœ€ç›¸å…³æ–‡æ¡£")
    logger.info("ğŸ“Š é‡æ’åºåˆ†æ•°:")
    for i, (score, chunk) in enumerate(reranked_results[:top_k_rerank]):
        logger.info(f"   {i+1}. åˆ†æ•°={score:.4f}")
    
    final_chunks = [chunk for score, chunk in reranked_results[:top_k_rerank]]
    logger.info(f"âœ… æ£€ç´¢å’Œé‡æ’åºå®Œæˆï¼Œè¿”å› {len(final_chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    
    return final_chunks

def build_prompt(question, options, context_chunks):
    logger.info(f"ğŸ“ æ„å»ºæç¤ºè¯ - é—®é¢˜: {question}")
    logger.info(f"ğŸ“š ä½¿ç”¨ {len(context_chunks)} ä¸ªä¸Šä¸‹æ–‡ç‰‡æ®µ")
    
    context = "\n\n".join(context_chunks)
    
    # è®°å½•ä¸Šä¸‹æ–‡ç»Ÿè®¡ä¿¡æ¯å’Œå†…å®¹
    logger.info(f"ğŸ“– ä¸Šä¸‹æ–‡ç»Ÿè®¡: å…± {len(context_chunks)} ä¸ªç‰‡æ®µ")
    logger.info("ğŸ“„ ä¸Šä¸‹æ–‡ç‰‡æ®µå†…å®¹:")
    for i, chunk in enumerate(context_chunks):
        logger.info(f"   ç‰‡æ®µ {i+1}: {chunk}")
    
    # æ„å»ºé€‰é¡¹åˆ—è¡¨
    options_text = "\n".join([f"{key}. {value}" for key, value in options.items()])
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„äº‹å®æ ¸æŸ¥åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„ã€ä¸Šä¸‹æ–‡ã€‘ä¿¡æ¯ï¼Œä»ç»™å®šçš„é€‰æ‹©é¢˜ä¸­é€‰æ‹©æœ€æ­£ç¡®çš„ç­”æ¡ˆã€‚

ä½ å¿…é¡»å®Œå…¨ä¸”ä»…æ ¹æ®ã€ä¸Šä¸‹æ–‡ã€‘ä¸­çš„ä¿¡æ¯æ¥åšå‡ºåˆ¤æ–­ã€‚ä¸¥ç¦ä½¿ç”¨ä»»ä½•ä½ çš„å…ˆéªŒçŸ¥è¯†ã€‚
è¿™ä¸€ç‚¹è‡³å…³é‡è¦ï¼Œå› ä¸ºéµå¾ªæ‰€ç»™æ–‡æœ¬æ˜¯è¯„ä¼°çš„ä¸¥æ ¼è§„åˆ™ã€‚ä»»ä½•è¶…å‡ºä¸Šä¸‹æ–‡çš„ä¿¡æ¯éƒ½ä¼šå¯¼è‡´é”™è¯¯çš„ç»“æœã€‚
ä¸æ­¤åŒæ—¶ï¼Œã€ä¸Šä¸‹æ–‡ã€‘å’Œã€é—®é¢˜ã€‘ä¸­ä¹Ÿæœ‰å¯èƒ½å‡ºç°å¤§é‡å†—ä½™ä¿¡æ¯ï¼Œè¯·åŠ¡å¿…ä»”ç»†ç”„åˆ«ï¼Œä¸è¦è¢«å†—ä½™ä¿¡æ¯æ‰€è¯¯å¯¼ã€‚

---
ã€ä¸Šä¸‹æ–‡ã€‘
{context}
---
ã€é—®é¢˜ã€‘
{question}

ã€é€‰é¡¹ã€‘
{options_text}
---

åŸºäºä½ çš„åˆ†æï¼Œè¯·ä»Aã€Bã€Cã€Dä¸­é€‰æ‹©æœ€æ­£ç¡®çš„ç­”æ¡ˆã€‚ä½ çš„ç­”æ¡ˆå¿…é¡»æ˜¯ä»¥ä¸‹å››ä¸ªé€‰é¡¹ä¹‹ä¸€ï¼š'A'ã€'B'ã€'C' æˆ– 'D'ã€‚

ç­”æ¡ˆï¼š
"""
    
    logger.info(f"âœ… æç¤ºè¯æ„å»ºå®Œæˆï¼Œæ€»é•¿åº¦: {len(prompt)} å­—ç¬¦")
    logger.info(f"PROMPT: {prompt}")
    return prompt

@app.post("/answer")
async def get_answer(query: Query):
    start_time = time.time()
    request_id = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    
    logger.info("=" * 80)
    logger.info(f"ğŸš€ æ–°è¯·æ±‚å¼€å§‹ - è¯·æ±‚ID: {request_id}")
    logger.info(f"ğŸ“ æ¥æ”¶åˆ°çš„è¾“å…¥æ•°æ®:")
    logger.info(f"   é—®é¢˜: {query.question}")
    logger.info(f"   é€‰é¡¹A: {query.option_A}")
    logger.info(f"   é€‰é¡¹B: {query.option_B}")
    logger.info(f"   é€‰é¡¹C: {query.option_C}")
    logger.info(f"   é€‰é¡¹D: {query.option_D}")
    
    question = query.question
    options = {
        "A": query.option_A,
        "B": query.option_B,
        "C": query.option_C,
        "D": query.option_D,
    }
    
    logger.info("ğŸ”„ å¼€å§‹å¤„ç†æ•´ä¸ªé—®é¢˜")
    
    # æ„å»ºå®Œæ•´çš„é—®é¢˜æ–‡æœ¬ç”¨äºæ£€ç´¢
    full_question = f"{question} {' '.join(options.values())}"
    logger.info(f"ğŸ“‹ å®Œæ•´é—®é¢˜æ–‡æœ¬: {full_question}")
    
    # æ£€ç´¢å’Œé‡æ’åº
    context_chunks = search_and_rerank(full_question)
    
    if not context_chunks:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œè¿”å›é»˜è®¤ç­”æ¡ˆA")
        final_answer = "A"
    else:
        # æ„å»ºæç¤ºè¯
        prompt = build_prompt(question, options, context_chunks)
        
        # LLM æ¨ç†
        logger.info("ğŸ¤– å¼€å§‹LLMæ¨ç†")
        llm_start_time = time.time()
        
        outputs = TEXT_GENERATOR(
            prompt,
            max_new_tokens=10,
            do_sample=False,
            temperature=0.0
        )
        
        llm_end_time = time.time()
        generated_text = outputs[0]['generated_text']
        answer = generated_text[len(prompt):].strip()
        
        logger.info(f"â±ï¸ LLMæ¨ç†è€—æ—¶: {llm_end_time - llm_start_time:.2f}ç§’")
        logger.info(f"ğŸ¯ LLMåŸå§‹è¾“å‡º: '{answer}'")
        
        # è§£æç­”æ¡ˆ
        final_answer = "A"  # é»˜è®¤ç­”æ¡ˆ
        for option_key in ["A", "B", "C", "D"]:
            if option_key in answer.upper():
                final_answer = option_key
                break
        
        logger.info(f"ğŸ† æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
    
    end_time = time.time()
    total_time = end_time - start_time
    logger.info(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
    logger.info(f"âœ… è¯·æ±‚å®Œæˆ - è¯·æ±‚ID: {request_id}")
    logger.info("=" * 80)
    
    return {"correct_answer": final_answer}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5000)